<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>CUDA Basics | Omar G. Younis</title> <meta name="author" content="Omar G. Younis"/> <meta name="description" content="Notes on CUDA"/> <meta name="keywords" content="machine-learning, phd, software, engineer"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ§ </text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://younis.dev/blog/cuda-basics/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://younis.dev/"><span class="font-weight-bold">Omar</span> G. Younis</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/activities/">activities</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">CUDA Basics</h1> <p class="post-meta">August 9, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a> Â  Â· Â  <a href="/blog/tag/CUDA"> <i class="fas fa-hashtag fa-sm"></i> CUDA</a> Â  Â  Â· Â  <a href="/blog/category/programming"> <i class="fas fa-tag fa-sm"></i> programming</a> Â  </p> </header> <article class="post-content"> <p>Recently, during my internship at EPFL, I worked on a CUDA kernel for matrix orthogonalization using QR decomposition. I was new to CUDA programming. Thus, I decided to write these quick notes for my future self to help refresh fundamental concepts.</p> <p>If you need to use your CUDA code with PyTorch, follow <a href="https://pytorch.org/tutorials/advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension" target="_blank" rel="noopener noreferrer">this guide</a> after reading this.</p> <h2 id="how-cuda-code-is-executed">How CUDA code is executed</h2> <p>The GPU can easily run multiple threads in parallel. Each thread needs to execute the same instruction. Thus, each thread reads the same intruction from a CUDA function (called kernel) and executes it. <br> What is the purpose of executing the same instruction multiple times in parallel? Well, the low-level instruction is the same, but each thread has some special registers with different values. With this difference as the starting point, each thread can read different pieces of memory, has different local results, etc.</p> <p>We will see an example soon, but first, let me define some important concepts:</p> <ul> <li> <strong>block</strong>: a group of threads. When we launch a CUDA kernel, we specify the number of blocks and how many threads each block has. Threads on the same block can communicate with a shared memory and synchronization primitives since they are executed on the same Streaming Multiprocessor (SM). A GPU has multiple SM thus, if the above features are not needed, we should consider packing the total number of threads in multiple blocks.</li> <li> <strong>warp</strong>: a group of 32 threads from the same block that are executed simultaneously. Not all threads that we create are executed simultaneously since we can create more threads than GPU cores. Threads are drawn out in groups of 32 (warp) to be executed together using the same clock. Thus, to maximize occupancy, we should define a number of threads per block that is a multiple of 32 (in practice a power of two, because the number of cores is always a power of two). Threads in the same warp share registers, allowing for some <a href="https://developer.nvidia.com/blog/using-cuda-warp-level-primitives/" target="_blank" rel="noopener noreferrer">warp-level primitives</a>.</li> </ul> <p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/Software-Perspective_for_thread_block.jpg/800px-Software-Perspective_for_thread_block.jpg" alt=""></p> <p>Letâ€™s do an easy example to show these concepts. The following kernel computes the sum of two vectors a, b, and saves the result in vector c.</p> <figure class="highlight"><pre><code class="language-cuda" data-lang="cuda"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td> <td class="code"><pre><span class="k">__global__</span> 
<span class="kt">void</span> <span class="nf">cuda_add</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">c</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">vecSize</span><span class="p">)</span> <span class="p">{</span>
  <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
  <span class="k">if</span><span class="p">(</span><span class="n">i</span> <span class="o">&lt;</span> <span class="n">vecSize</span><span class="p">){</span>
    <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>where <em>blockDim</em>, <em>blockIdx</em>, and <em>threadIdx</em> indicates the number of thread per block, the index of the block, and the index of the thread respectively. Those dimensions can be 3-dimensional; in this simple case, we can just work with the x-axis, with a 1-D number of blocks and threads per block. <br> The keyword <code class="language-plaintext highlighter-rouge">__global__</code> indicates that this function is a CUDA kernel and it can be launched from the CPU. Notice that the <code class="language-plaintext highlighter-rouge">__global__</code> function must be void, thus we should work with C++ pointers for the output. This is the common scenario: the CPU allocates and copies the input into the GPU memory, launches the kernel, wait for the completion, and reads the result from the output memory. Thus, we can launch the kernel with a function similar to the following one:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre></td> <td class="code"><pre> <span class="kt">float</span><span class="o">*</span> <span class="nf">add</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="k">const</span> <span class="kt">float</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="k">const</span> <span class="kt">int</span> <span class="n">vecSize</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">d_a</span><span class="p">,</span> <span class="o">*</span><span class="n">d_b</span><span class="p">,</span> <span class="o">*</span><span class="n">d_c</span><span class="p">;</span> <span class="c1">// device (GPU) vectors</span>

    <span class="kt">size_t</span> <span class="n">memSize</span> <span class="o">=</span> <span class="n">vecSize</span> <span class="o">*</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">);</span>
    
    <span class="c1">// allocate memory in GPU</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span> <span class="n">memSize</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_b</span><span class="p">,</span> <span class="n">memSize</span><span class="p">);</span>
    <span class="n">cudaMalloc</span><span class="p">((</span><span class="kt">void</span> <span class="o">**</span><span class="p">)</span><span class="o">&amp;</span><span class="n">d_c</span><span class="p">,</span> <span class="n">memSize</span><span class="p">);</span>

    <span class="c1">// copy inputs from CPU to GPU</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">memSize</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_b</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">memSize</span><span class="p">,</span> <span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

    <span class="c1">// launch the kernel</span>
    <span class="kt">int</span> <span class="n">threadsPerBlock</span> <span class="o">=</span> <span class="mi">256</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">blocksPerGrid</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">vecSize</span> <span class="o">/</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">threadsPerBlock</span> <span class="p">);</span> <span class="c1">// include math.h</span>
    <span class="n">cuda_add</span><span class="o">&lt;&lt;&lt;</span><span class="n">blocksPerGrid</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span> <span class="n">d_b</span><span class="p">,</span> <span class="n">d_c</span><span class="p">,</span> <span class="n">vecSize</span><span class="p">);</span>

    <span class="c1">// free memory</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_a</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_b</span><span class="p">);</span>

    <span class="c1">// allocate CPU memory for result and copy data from GPU</span>
    <span class="kt">float</span> <span class="o">*</span><span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">memSize</span><span class="p">);</span>
    <span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">d_c</span><span class="p">,</span> <span class="n">memSize</span><span class="p">,</span> <span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
    <span class="n">cudaFree</span><span class="p">(</span><span class="n">d_c</span><span class="p">);</span>

    <span class="k">return</span> <span class="n">c</span><span class="p">;</span>
<span class="p">}</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>In the above example, we launch the kernel with 256 threads per block and a number of block that depends on the <em>vecSize</em>, so we have at least one thread per element. The best number of thread per block depends on the GPU and the application: it should be tuned empirically. The maximum number in recent architecure is 1024, thus it usually suffice to try few power of two to find the best value. In this example, each element of the resulting vector <em>c</em> has a thread to compute the result. For some input dimension, it is faster to have a thread handling multiple sum with a for loop. In this case, for best performances, each warp should access to a contiguous memory in order to be able to exploit the caching mechanism; we will give an example in the later section. CUDA operations are non-blocking, but they are scheduled in a <em>stream</em>. It means that line 25 might be executed before the <code class="language-plaintext highlighter-rouge">cuda_add</code> kernel completion. The GPU executes the operation in the stream sequentially, so we are sure that <code class="language-plaintext highlighter-rouge">cudaFree</code> lines are executed only after the completion of the kernel. Instead, <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> is a blocking operation (with <code class="language-plaintext highlighter-rouge">cudaMemcpyAsync</code> the non-blocking counterpart), thus the result we return will be consistent. We can schedule CUDA operations on different streams to increase parallelism when there arenâ€™t ordering contraint. We can block the CPU, waiting for operation on the GPU using the function <code class="language-plaintext highlighter-rouge">cudaDeviceSynchronize</code> or <code class="language-plaintext highlighter-rouge">cudaStreamSynchronize</code>.</p> <h2 id="reduction-pattern">Reduction pattern</h2> <p>We showed a simple pattern, where every element of the resulting vector can be computed independently from the others. Another common pattern is <em>reduction</em>, when partial results should be combined. The simplest example is the dot product, where the kernel should sum the product of elements. Writing an efficient kernel for reduction is not trivial (as starting point, you can have a look at <a href="https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf" target="_blank" rel="noopener noreferrer">these slides</a>), but fortunately, we can rely on libraries such as <em><a href="https://docs.nvidia.com/cuda/cublas/index.html" target="_blank" rel="noopener noreferrer">cuBLAS</a></em> or <em><a href="https://nvlabs.github.io/cub/" target="_blank" rel="noopener noreferrer">CUB</a></em>. The first one contains high performant kernel for common linear algebra operations. For example, to compute the dot product, we can write something similar to this:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre></td> <td class="code"><pre><span class="cp">#include</span> <span class="cpf">&lt;cublas_v2.h&gt;</span><span class="cp">
</span>
<span class="kt">int</span> <span class="nf">main</span><span class="p">(){</span>
    <span class="c1">// create the cuBLAS library context</span>
    <span class="n">cublasHandle_t</span> <span class="n">h</span><span class="p">;</span>
    <span class="n">cublasCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">h</span><span class="p">);</span>

    <span class="p">...</span> <span class="c1">// do something, create the cuda vectors d_a, d_b</span>

    <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">cublasDdot</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">vecSize</span><span class="p">,</span> <span class="n">d_a</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">result</span><span class="p">);</span>

    <span class="p">...</span> <span class="c1">// do soething else, use the result stored in the CPU variable result</span>

    <span class="c1">// destroy cuBLAS context</span>
    <span class="n">cublasDestroy</span><span class="p">(</span><span class="n">h</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>If you want to write a custom kernel that includes reductions along with other operations, you can rely on the CUB library, which contains useful block-wide and warp-wide primitive functions. It means, that instead of calling the function from the CPU, we can call it inside a kernel, where every thread calls it. For example, we can write a dot function:</p> <figure class="highlight"><pre><code class="language-c--" data-lang="c++"><table class="rouge-table"><tbody><tr>
<td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre></td> <td class="code"><pre><span class="cp">#include</span> <span class="cpf">&lt;cub/cub.cuh&gt;</span><span class="cp">
</span>
<span class="n">__global__</span> 
<span class="kt">float</span> <span class="nf">dot</span><span class="p">(</span><span class="kt">float</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="kt">float</span> <span class="o">*</span><span class="n">b</span><span class="p">,</span> <span class="kt">int</span> <span class="n">vecSize</span><span class="p">){</span>
    <span class="kt">int</span> <span class="n">tx</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">unroll</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">vecSize</span> <span class="o">/</span> <span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="p">);</span>
    <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">tx</span> <span class="o">&amp;</span> <span class="o">-</span><span class="mi">32u</span><span class="p">)</span> <span class="o">*</span> <span class="n">unroll</span> <span class="o">+</span> <span class="p">(</span><span class="n">tx</span> <span class="o">&amp;</span> <span class="mi">31</span><span class="p">);</span>

    <span class="kt">float</span> <span class="n">localProd</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">unroll</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">){</span>
          <span class="n">localProd</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">idx</span><span class="p">];</span>
        <span class="p">}</span>
        <span class="n">idx</span> <span class="o">+=</span> <span class="mi">32</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="kt">int</span> <span class="n">nWarp</span> <span class="o">=</span> <span class="n">ceil</span><span class="p">(</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">/</span> <span class="mf">32.0</span><span class="p">);</span>
    <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">tmp_storage</span><span class="p">[</span><span class="n">nWarp</span><span class="p">];</span>
    <span class="kt">float</span> <span class="n">reduce</span> <span class="o">=</span> <span class="n">cub</span><span class="o">::</span><span class="n">BlockReduceSum</span><span class="p">(</span><span class="n">localProd</span><span class="p">,</span> <span class="n">tmp_storage</span><span class="p">);</span>

    <span class="c1">// reduce contains the correct result only on thread 0</span>
     <span class="n">__shared__</span> <span class="kt">float</span> <span class="n">dot</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">tx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> 
        <span class="n">dot</span> <span class="o">=</span> <span class="n">reduce</span><span class="p">;</span>
    <span class="n">__syncthreads</span><span class="p">();</span>

    <span class="k">return</span> <span class="n">dot</span><span class="p">;</span>
<span class="p">}</span>
</pre></td> </tr></tbody></table></code></pre></figure> <p>Remembering that <code class="language-plaintext highlighter-rouge">&amp;</code> is the bitwise and operation, <code class="language-plaintext highlighter-rouge">tx &amp; 31</code> mask every bit except the last 5, thus it computes the modulo 32 operation of <code class="language-plaintext highlighter-rouge">tx</code>. Instead, <code class="language-plaintext highlighter-rouge">tx &amp; -32u</code> takes the other bits, thus it is the warp index multiplied by 32. With this in mind, line 7 computes the index for the current thread so warps can be unrolled on a contiguous memory, as shown in the following figure, where we have a vector of 128 elements and 64 threads, thus 2 warps and <code class="language-plaintext highlighter-rouge">unroll = 2</code>. In the first iteration of the loop in line 10, each thread of the first warp compute the <code class="language-plaintext highlighter-rouge">localProd</code> of the elements from 0 to 31. In the second iteration, they slide by 32 position and accumulate the product of elements.</p> <figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/warp-unroll-480.webp"></source> <source media="(max-width: 800px)" srcset="/assets/img/warp-unroll-800.webp"></source> <source media="(max-width: 1400px)" srcset="/assets/img/warp-unroll-1400.webp"></source> <img class="img-fluid rounded" src="/assets/img/warp-unroll.jpg"> </picture> </figure> <p>After the for loop, each thread has a local variable <code class="language-plaintext highlighter-rouge">localProd</code> with the sum of the product of <code class="language-plaintext highlighter-rouge">unroll</code> elements; the sum of all <code class="language-plaintext highlighter-rouge">localProd</code> will give us the <code class="language-plaintext highlighter-rouge">dot</code> product of the two input vectors. To achieve this, we can rely on the <code class="language-plaintext highlighter-rouge">BlockReduceSum</code> provided by CUB that should be called by each thread with the local value and shared memory to store partial results. The shared memory is allocated using the keyword <code class="language-plaintext highlighter-rouge">__shared__</code> and it is a memory accessible by all threads of the block. As you may have noticed, to support reduction we allocated a shared array that is long as the number of warps. Thread on the same warp can reduce their local value with faster registers (see <a href="https://on-demand.gputechconf.com/gtc/2013/presentations/S3174-Kepler-Shuffle-Tips-Tricks.pdf" target="_blank" rel="noopener noreferrer">these slides</a>), while different warps need to communicate through shared memory. The value returned by <code class="language-plaintext highlighter-rouge">BlockReduceSum</code> will be the correct result for thread 0 but will be a partial result for other threads. If we need the correct result in all threads, we can rely again on a shared value and let the first thread store the value there. Since different warps can be executed independently, we need to synchronize threads after the assignment; we can do it with the primitive <code class="language-plaintext highlighter-rouge">__syncthreads()</code>. In this way, every thread will wait until all the others in the same block reach this line. You may wonder how it is possible to have an <code class="language-plaintext highlighter-rouge">if</code> statement when all threads in a warp execute the same instruction simultaneously. This is handled by the compiler: other threads will execute <code class="language-plaintext highlighter-rouge">NOP</code> while thread 0 executes the branch. This is called <em>branch divergence</em> and, to improve performances, it should be avoided when possible.</p> <h2 id="useful-resources">Useful resources</h2> <p>There is much more to say about CUDA. A complete starting point is the <a href="https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf" target="_blank" rel="noopener noreferrer">CUDA programming guide</a>. Other specific topics can be found on the <a href="https://docs.nvidia.com/cuda/index.html" target="_blank" rel="noopener noreferrer">documentation page</a>. University courses exist and provide material for a softer starting point, for example, the slides from <a href="https://people.maths.ox.ac.uk/gilesm/cuda/" target="_blank" rel="noopener noreferrer">this Oxford course</a>, that I report here for convenience:</p> <ul> <li><a href="https://people.maths.ox.ac.uk/gilesm/cuda/2019/lecture_01.pdf" target="_blank" rel="noopener noreferrer">An introduction to CUDA</a></li> <li><a href="https://people.maths.ox.ac.uk/gilesm/cuda/2019/lecture_02.pdf" target="_blank" rel="noopener noreferrer">Different memory and variable types</a></li> <li><a href="https://people.maths.ox.ac.uk/gilesm/cuda/2019/lecture_03.pdf" target="_blank" rel="noopener noreferrer">Control flow and synchronisation</a></li> <li><a href="https://people.maths.ox.ac.uk/gilesm/cuda/2019/lecture_04.pdf" target="_blank" rel="noopener noreferrer">Warp shuffles, and reduction / scan operations</a></li> <li><a href="https://people.maths.ox.ac.uk/gilesm/cuda/2019/lecture_05.pdf" target="_blank" rel="noopener noreferrer">Libraries and tools</a></li> <li><a href="https://people.maths.ox.ac.uk/gilesm/cuda/2019/lecture_06.pdf" target="_blank" rel="noopener noreferrer">Multiple GPUs, and odds and ends</a></li> <li><a href="https://people.maths.ox.ac.uk/gilesm/cuda/2019/lec7.pdf" target="_blank" rel="noopener noreferrer">Tackling a new CUDA application</a></li> <li><a href="https://people.maths.ox.ac.uk/gilesm/cuda/2019/CUDA_Course_profiling2.pdf" target="_blank" rel="noopener noreferrer">Profiling and tuning applications</a></li> </ul> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> Â© Copyright 2022 Omar G. Younis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. VAT 04387850987 </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>